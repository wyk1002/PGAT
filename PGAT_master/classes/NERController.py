import pickle
import os
import sys
import matplotlib.pyplot as pl
import json
import gc
import torch.autograd as autograd
import torch.optim as optim
import time
from .trie import Trie
import numpy as np
import torch
from transformers import BertTokenizer
from transformers import BertModel
from .TrainModel import PGAT_Model
from sklearn import model_selection, metrics


class NERController:
    def __init__(self, charWindowSize, maxSentenceLength, lr, decay_rate, epoch, batchSize, pgat_head, att_head,
                 trust_rank,
                 hidden_dim, dropout, att_dropout, dropout_trainable, use_gpu, use_bert, source_type,
                 which_step_to_print, layer_num, seq_head,
                 char_emb_file, word_emb_file, seq_feature_model, context_feature_model, seq_encoding_model,
                 cuda_version, random_seed, optimizer):
        self.charWindowSize = charWindowSize
        self.maxSentenceLength = maxSentenceLength
        self.lr = lr
        self.decay_rate = decay_rate
        self.epoch = epoch
        self.seq_head = seq_head
        self.seq_feature_model = seq_feature_model
        self.batchSize = batchSize
        self.layer_num = layer_num
        self.char_emb_file = char_emb_file
        self.use_gpu = use_gpu
        self.word_emb_file = word_emb_file
        self.trust_rank = trust_rank
        if self.word_emb_file == 'ctb.50d.vec':
            self.d_word = 50
        elif self.word_emb_file == 'sgns.merge.word':
            self.d_word = 300
        if self.char_emb_file == 'gigaword_chn.all.a2b.uni.ite50.vec':
            self.d_char = 50
        elif self.char_emb_file == 'bert':
            self.d_char = 768
            self.char_dict = None
        self.pgat_head = pgat_head
        self.att_head = att_head
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        self.optimizer = optimizer
        self.att_dropout = att_dropout
        self.dropout_trainable = dropout_trainable
        self.use_bert = use_bert
        self.source_type = source_type
        self.which_step_to_print = which_step_to_print
        self.seq_encoding_model = seq_encoding_model
        self.context_feature_model = context_feature_model
        self.random_seed = random_seed

        self.char_dict = None
        self.word_dict = None
        self.char2id = {'PAD': 0}
        self.id2char = {0: 'PAD'}
        self.word2id = {'PAD': 0}
        self.id2word = {0: 'PAD'}
        self.pretrain_char_emb = [[]]
        self.pretrain_word_emb = [[]]
        self.trie = None
        self.bert_tokenizer = None

        self.tagScheme = 'BMES'
        if source_type == 'ecommerce':
            self.tagScheme = 'BIES'
        if source_type == 'msra':
            self.none_dev = True

        self.result_path = r'./result/'
        self.emb_dic_path = r'./cache/emb_dic/'
        self.input_data_path = r'./cache/input_data/'
        self.resource_path = r'./resource/'
        self.variable_path = r'./cache/variable/'
        self.word2vec_path = r'./cache/word2vec/'
        self.inf_path = r'./cache/inf/'

        if self.use_bert:
            self.variable_path += 'bert/'
        else:
            self.variable_path += 'base/'

        # GPU information
        os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
        os.environ["CUDA_VISIBLE_DEVICES"] = '0'
        # self.gpu = torch.cuda.is_available()
        # gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
        # tf.config.experimental.set_memory_growth(gpus[cuda_version], True)
        # tf.config.experimental.set_visible_devices(devices=gpus[cuda_version], device_type='GPU')

    def test(self):
        self.init_ctg()
        chars_train, words_train, bert_train, y_train, G_train, char_lens_train, word_lens_train, chars_dev, \
        words_dev, bert_dev, y_dev, G_dev, char_lens_dev, word_lens_dev, chars_test, words_test, bert_test, \
        y_test, G_test, char_lens_test, word_lens_test, self.char2id, self.id2char, self.pretrain_char_emb, \
        self.word2id, self.id2word, self.pretrain_word_emb = self.get_input_data()

        model = PGAT_Model(self.d_char, self.d_word, self.hidden_dim, self.dropout, self.att_dropout, self.trust_rank,
                           self.layer_num, self.category_num, self.seq_encoding_model, self.pgat_head, self.seq_head,
                           self.att_head, self.context_feature_model, self.seq_feature_model, self.use_gpu,
                           self.use_bert, self.char_emb_file == 'bert', self.pretrain_char_emb, self.pretrain_word_emb)

        variable_file = self.variable_path + self.source_type

        model.load_state_dict(torch.load(variable_file))

        gold_num, predict_num, right_num, acc, p_test, r_test, f_test, test_times, time_list = self.test_P_GAT(
            self.batchSize,
            chars_test,
            words_test,
            bert_test,
            y_test, G_test,
            char_lens_test,
            word_lens_test,
            model, True)
        time_cost = self.deal_time(time_list)
        print(
            'Test %d instances in %.2fs, speed: %.2fst/s' % (len(chars_test), test_times, len(chars_test) / test_times))
        print('Average time cost split by {0,20,40,60,80,large} is (s/sent):', time_cost)
        print('gold_num:%5d  predict_num:%5d  right_num:%5d' % (gold_num, predict_num, right_num))
        print('Test result: p=%.4f  r=%.4f  f=%.4f' % (p_test, r_test, f_test))
        # self.get_metrics(predict_list,y_test[0:-(data_length % self.batchSize)],char_lens_test[0:-(data_length % self.batchSize)])

    def train(self):
        self.init_ctg()
        chars_train, words_train, bert_train, y_train, G_train, char_lens_train, word_lens_train, chars_dev, \
        words_dev, bert_dev, y_dev, G_dev, char_lens_dev, word_lens_dev, chars_test, words_test, bert_test, \
        y_test, G_test, char_lens_test, word_lens_test, self.char2id, self.id2char, self.pretrain_char_emb, \
        self.word2id, self.id2word, self.pretrain_word_emb = self.get_input_data()

        model = PGAT_Model(self.d_char, self.d_word, self.hidden_dim, self.dropout, self.att_dropout, self.trust_rank,
                           self.layer_num, self.category_num, self.seq_encoding_model, self.pgat_head, self.seq_head,
                           self.att_head, self.context_feature_model, self.seq_feature_model, self.use_gpu,
                           self.use_bert, self.char_emb_file == 'bert', self.pretrain_char_emb, self.pretrain_word_emb)
        nan_flag = False
        max_test_p = -1
        max_test_r = -1
        max_test_f = -1
        p = []
        r = []
        f = []

        if self.random_seed != None:
            np.random.seed(self.random_seed)
            np.random.shuffle(chars_train)
            np.random.seed(self.random_seed)
            np.random.shuffle(words_train)
            np.random.seed(self.random_seed)
            np.random.shuffle(bert_train)
            np.random.seed(self.random_seed)
            np.random.shuffle(y_train)
            np.random.seed(self.random_seed)
            np.random.shuffle(G_train)
            np.random.seed(self.random_seed)
            np.random.shuffle(char_lens_train)
            np.random.seed(self.random_seed)
            np.random.shuffle(word_lens_train)

        self.clear_cache()

        parameters = filter(lambda p: p.requires_grad, model.parameters())
        if self.optimizer == 'Adamax':
            optimizer = optim.Adamax(parameters, lr=self.lr)
        if self.optimizer == 'Adam':
            optimizer = optim.Adam(parameters, lr=self.lr)
        if self.optimizer == 'SGD':
            optimizer = optim.SGD(parameters, lr=self.lr)
        print("Data is readyÔºÅ Start training...")

        modelFile = self.variable_path + self.source_type

        def lr_decay(optimizer, epoch, decay_rate=self.decay_rate, init_lr=self.lr):
            lr = init_lr * ((1 - decay_rate) ** epoch)
            print("Epoch %d: Learning rate is setted as: %.5f" % (epoch, lr))
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr
            return optimizer

        # train************************************************************************************
        train_size = len(chars_train)
        for epoch in range(self.epoch):
            model.train()
            optimizer = lr_decay(optimizer, epoch)
            start_index = 0
            right_tag_num = 0
            total_tag_num = 0
            total_loss = 0
            start_time = time.time()
            while (start_index < train_size):
                end_index = start_index + self.batchSize
                if end_index > train_size:
                    end_index = train_size
                chars = chars_train[start_index:end_index]
                words = words_train[start_index:end_index]
                bert = bert_train[start_index:end_index]
                label = y_train[start_index:end_index]
                G = G_train[start_index:end_index]
                char_lens = char_lens_train[start_index:end_index]
                word_lens = word_lens_train[start_index:end_index]
                chars, words, bert, bert_mask, label, lens_mask, G, self_mask_matrix = self.get_bitcher(chars, words,
                                                                                                        bert, label,
                                                                                                        G,
                                                                                                        char_lens,
                                                                                                        word_lens)

                loss, predict = model.call(chars, words, bert, bert_mask, label, self_mask_matrix, lens_mask, G)
                right, whole = self.predict_check(predict, label, lens_mask)
                total_loss += loss.data
                right_tag_num += right
                total_tag_num += whole
                if loss.isnan():
                    print('loss is nan, interrupt training!')
                    nan_flag = True
                    break

                loss.backward()
                optimizer.step()
                model.zero_grad()
                if (end_index // self.batchSize) % self.which_step_to_print == 0 or end_index == train_size:
                    end_time = time.time()
                    print(
                        "    epoch=%2d  And  step=%5d  time_cost=%5.2fs-> lossÔºö%10.4f  acc=%s/%s=%.4f" % (
                            epoch, end_index // self.batchSize, end_time - start_time, total_loss, right_tag_num,
                            total_tag_num, (right_tag_num + 0.) / total_tag_num))
                    right_tag_num = 0
                    total_tag_num = 0
                    total_loss = 0
                    start_time = end_time

                    sys.stdout.flush()  # Âà∑Êñ∞ÁºìÂ≠òÂå∫
                start_index = end_index

            if nan_flag:
                print('The best result isÔºöp=%.4f  r=%.4f  f=%.4f' % (max_test_p, max_test_r, max_test_f))
                break
            # dev
            if words_dev != None:
                gold_num, predict_num, right_num, acc, p_dev, r_dev, f_dev, dev_times = self.test_P_GAT(1, chars_dev,
                                                                                                        words_dev,
                                                                                                        bert_dev,
                                                                                                        y_dev, G_dev,
                                                                                                        char_lens_dev,
                                                                                                        word_lens_dev,
                                                                                                        model, False)
                print(
                    'dev  %d instances, time cost %5.2fs, speed:%5.2fsen/s gold_num=%5d,predict_num=%5d,right_num=%5d -->'
                    ' acc=%.4f  p=%.4f  r=%.4f  f=%.4f' % (
                        len(chars_dev), dev_times, len(chars_dev) / dev_times, gold_num, predict_num,
                        right_num, acc, p_dev, r_dev, f_dev))

            # test
            gold_num, predict_num, right_num, acc, p_test, r_test, f_test, test_times = self.test_P_GAT(1, chars_test,
                                                                                                        words_test,
                                                                                                        bert_test,
                                                                                                        y_test, G_test,
                                                                                                        char_lens_test,
                                                                                                        word_lens_test,
                                                                                                        model, False)
            print('test %d instances, time cost %5.2fs, speed:%5.2fsen/s gold_num=%5d,predict_num=%5d,right_num=%5d -->'
                  ' acc=%.4f  p=%.4f  r=%.4f  f=%.4f' % (
                      len(chars_test), test_times, len(chars_test) / test_times, gold_num, predict_num,
                      right_num, acc, p_test, r_test, f_test))
            p.append(p_test)
            r.append(r_test)
            f.append(f_test)
            if f_test > max_test_f:
                max_test_f = f_test
                max_test_p = p_test
                max_test_r = r_test
                torch.save(model.state_dict(), modelFile)
            gc.collect()  # Ê∏ÖÁêÜÂÜÖÂ≠ò

            print('The best test resultÔºöp=%.4f  r=%.4f  f=%.4f' % (max_test_p, max_test_r, max_test_f))
            print('******************************************************************')
        self.draw_pic(p, r, f)
        with open(self.result_path + self.source_type + '/' + self.source_type + '_para.txt', "a") as f:
            f.write(modelFile + '\n')
            f.write(
                "Test score: p:%.4f, r:%.4f, f:%.4f\n b=%d, lr=%.6f, de_lr=%.4f, optimizer=%s, h=%d, dropout=%.2f, att_dropout=%.2f, layer=%d\n\n" % (
                    max_test_p, max_test_r, max_test_f, self.batchSize, self.lr, self.decay_rate, self.optimizer,
                    self.hidden_dim, self.dropout,
                    self.att_dropout, self.layer_num))
            f.close()
        print('train overÔºÅ')

    def test_P_GAT(self, b, chars_test, words_test, char_bert, y_test, G_test, char_lens_test, word_lens_test, model,
                   return_timelist):
        model.eval()
        time_list = []
        predict_list = []
        data_length = len(chars_test)
        start_index = 0
        total_time = 0
        while (start_index < data_length):
            with torch.no_grad():
                end_index = start_index + b
                if end_index > data_length:
                    end_index = data_length
                chars = chars_test[start_index:end_index]
                words = words_test[start_index:end_index]
                if self.use_bert:
                    bert = char_bert[start_index:end_index]
                else:
                    bert = None
                if y_test != None:
                    label = y_test[start_index:end_index]
                else:
                    label = None
                G = G_test[start_index:end_index]
                char_lens = char_lens_test[start_index:end_index]
                word_lens = word_lens_test[start_index:end_index]

                start_time = time.time()
                chars, words, bert, bert_mask, label, lens_mask, G, self_mask_matrix = self.get_bitcher(chars, words,
                                                                                                        bert, label,
                                                                                                        G,
                                                                                                        char_lens,
                                                                                                        word_lens)
                predict = model(chars, words, bert, bert_mask, self_mask_matrix, lens_mask, G)
                end_time = time.time()
                predict_list.extend(predict)

                time_list.append([max(char_lens), b / (end_time - start_time)])
                total_time += (end_time - start_time)
                start_index = end_index

        gold_num, predict_num, right_num, acc, p_test, r_test, f_test = self.get_score(predict_list, y_test,
                                                                                       char_lens_test)
        if return_timelist:
            return gold_num, predict_num, right_num, acc, p_test, r_test, f_test, total_time, time_list
        else:

            return gold_num, predict_num, right_num, acc, p_test, r_test, f_test, total_time

    def load_file(self, file):
        split_char = ' '
        if self.source_type == 'ecommerce':
            split_char = '\t'
        sentences = []
        labels = []

        sentence = []
        label = []
        for line in open(file, 'r', encoding='utf-8').readlines():
            if len(line) > 2:
                data = line.rstrip().split(split_char)
                sentence.append(data[0])
                label.append(self.ctgtonum(data[1]))
            elif len(sentence) > 0:
                if len(sentence) > self.maxSentenceLength:
                    sentence = sentence[:self.maxSentenceLength]
                    label = label[:self.maxSentenceLength]
                sentences.append(sentence)
                labels.append(label)
                sentence = []
                label = []
        return sentences, labels

    def get_trie(self):
        file = r'./cache/gaz_embedding/' + self.word_emb_file
        if os.path.exists(self.inf_path + 'trie_' + str(self.d_word) + '.pkl') == True:
            self.trie = self.read_file(self.inf_path + 'trie_' + str(self.d_word) + '.pkl')
        else:
            print('building Tire...')
            self.trie = Trie()
            lines = open(file, 'r', encoding="utf-8").readlines()
            for line in lines:
                data = line.strip().split(' ')
                self.trie.insert(data[0])
            self.write_file(self.inf_path + 'trie_' + str(self.d_word) + '.pkl', self.trie)

    def get_emb_dic_by_gaz(self):
        word_file = self.emb_dic_path + 'word_gaz_dict_' + str(self.d_word) + '.txt'
        char_file = self.emb_dic_path + 'char_gaz_dict_' + str(self.d_char) + '.txt'
        if os.path.exists(word_file) == True:
            with open(word_file, 'r', encoding='utf-8') as fp:
                self.word_dict = json.load(fp)
        else:
            print('building word embedding dict...')
            self.word_dict = {}
            lines = open(r'./cache/gaz_embedding/' + self.word_emb_file, 'r', encoding="utf-8").readlines()
            if self.word_emb_file == 'sgns.merge.word':
                lines = lines[1:]
            for line in lines:
                data = line.strip().split(' ')
                if len(data) == self.d_word + 1:
                    embedd = np.empty([self.d_word])
                    embedd[:] = data[1:]
                    word = data[0]
                    if word not in self.word_dict:
                        self.word_dict[word] = self.norm2one(embedd).tolist()

            with open(word_file, 'w', encoding='utf-8') as fp:
                json.dump(self.word_dict, fp, ensure_ascii=False)

        if self.char_emb_file != 'bert':
            if os.path.exists(char_file) == True:
                with open(char_file, 'r', encoding='utf-8') as fp:
                    self.char_dict = json.load(fp)
            else:
                print('building char embedding dict...')
                self.char_dict = {}
                lines = open(r'./cache/gaz_embedding/' + self.char_emb_file, 'r', encoding="utf-8").readlines()
                for line in lines:
                    data = line.strip().split(' ')
                    if len(data) == self.d_char + 1:
                        embedd = np.empty([self.d_char])
                        embedd[:] = data[1:]
                        char = data[0]
                        if char not in self.char_dict:
                            self.char_dict[char] = self.norm2one(embedd).tolist()
                with open(char_file, 'w', encoding='utf-8') as fp:
                    json.dump(self.char_dict, fp, ensure_ascii=False)

    def deal_time(self, data):
        result = [[] for i in range(5)]
        time_cost = [0 for i in range(5)]
        for i in range(len(data)):
            if data[i][0] < 20:
                result[0].append(data[i][1])
            elif data[i][0] < 40:
                result[1].append(data[i][1])
            elif data[i][0] < 60:
                result[2].append(data[i][1])
            elif data[i][0] < 80:
                result[3].append(data[i][1])
            else:  # len>=80
                result[4].append(data[i][1])
        for i in range(len(result)):
            if len(result[i]) != 0:
                time_cost[i] = sum(result[i]) / len(result[i])
        return time_cost

    def init_ctg(self):
        if self.source_type == 'ontonote':
            self.ctg_dic = ['O', 'B-GPE', 'M-GPE', 'E-GPE', 'B-LOC', 'M-LOC', 'E-LOC', 'B-ORG', 'M-ORG', 'E-ORG',
                            'B-PER', 'M-PER', 'E-PER', 'S-GPE', 'S-LOC', 'S-ORG', 'S-PER']
        elif self.source_type == 'resume':
            self.ctg_dic = ['O', 'B-CONT', 'M-CONT', 'E-CONT', 'B-EDU', 'M-EDU', 'E-EDU', 'B-LOC', 'M-LOC', 'E-LOC',
                            'B-NAME', 'M-NAME', 'E-NAME', 'B-ORG', 'M-ORG', 'E-ORG', 'B-PRO', 'M-PRO', 'E-PRO',
                            'B-RACE', 'M-RACE', 'E-RACE', 'B-TITLE', 'M-TITLE', 'E-TITLE', 'S-NAME', 'S-ORG',
                            'S-RACE', ]
        elif self.source_type == 'ecommerce':
            self.ctg_dic = ['O', 'B-HC', 'I-HC', 'E-HC', 'B-HP', 'I-HP', 'E-HP', 'S-HC', 'S-HP']
        elif self.source_type == 'weibo_all':
            self.ctg_dic = ['O', 'B-GPE.NAM', 'M-GPE.NAM', 'E-GPE.NAM', 'B-LOC.NAM', 'M-LOC.NAM', 'E-LOC.NAM',
                            'B-ORG.NAM', 'M-ORG.NAM', 'E-ORG.NAM', 'B-PER.NAM', 'M-PER.NAM', 'E-PER.NAM', 'B-GPE.NOM',
                            'E-GPE.NOM', 'B-LOC.NOM', 'M-LOC.NOM', 'E-LOC.NOM', 'B-ORG.NOM', 'M-ORG.NOM', 'E-ORG.NOM',
                            'B-PER.NOM', 'M-PER.NOM', 'E-PER.NOM', 'S-GPE.NAM', 'S-PER.NAM', 'S-LOC.NOM', 'S-PER.NOM']
        elif self.source_type == 'msra':
            self.ctg_dic = ['O', 'B-NR', 'M-NR', 'E-NR', 'B-NS', 'M-NS', 'E-NS', 'B-NT', 'M-NT', 'E-NT', 'S-NR', 'S-NS',
                            'S-NT']
        elif self.source_type == 'demo':
            self.ctg_dic = ['O', 'B-GPE', 'M-GPE', 'E-GPE', 'B-LOC', 'M-LOC', 'E-LOC', 'B-ORG', 'M-ORG', 'E-ORG',
                            'B-PER', 'M-PER', 'E-PER', 'S-GPE', 'S-LOC', 'S-ORG', 'S-PER']

        self.category_num = len(self.ctg_dic)

    def norm2one(self, vec):
        root_sum_square = np.sqrt(np.sum(np.square(vec)))
        return vec / root_sum_square

    def get_bitcher(self, chars, words, bert, label, G, char_lens, word_lens):
        b = len(chars)
        max_char_len = max(char_lens)
        max_word_len = max(word_lens)
        if self.char_emb_file != 'bert':
            char_bitcher = autograd.Variable(torch.zeros((b, max_char_len))).long()
        else:
            char_bitcher = None
        word_bitcher = autograd.Variable(torch.zeros([b, max_word_len])).long()
        if self.char_emb_file == 'bert' or self.use_bert == True:
            bert_bitcher = autograd.Variable(torch.zeros([b, max_char_len + 2])).long()
            bert_mask = autograd.Variable(torch.zeros((b, max_char_len + 2))).long()
        else:
            bert_bitcher = None
            bert_mask = None
        label_bitcher = autograd.Variable(torch.zeros([b, max_char_len])).long()
        G_bitcher = autograd.Variable(torch.zeros([b, 4, max_char_len, max_word_len])).byte()
        mask_bitcher = autograd.Variable(torch.zeros([b, max_char_len])).byte()
        self_mask_bitcher = autograd.Variable(torch.zeros([b, max_char_len, max_char_len])).byte()

        for i in range(b):
            char_len = char_lens[i]
            word_len = word_lens[i]
            if self.char_emb_file != 'bert':
                char_bitcher[i, :char_len] = torch.LongTensor(chars[i])
            word_bitcher[i, :word_len] = torch.LongTensor(words[i])
            if self.char_emb_file == 'bert' or self.use_bert == True:
                bert_bitcher[i, :char_len + 2] = torch.LongTensor(bert[i])
                bert_mask[i, :char_len + 2] = torch.LongTensor([1] * int(char_len + 2))
            label_bitcher[i, :char_len] = torch.LongTensor(label[i])
            mask_bitcher[i, :char_len] = torch.Tensor([1] * char_len)
            self_mask_bitcher[i, :char_len, :char_len] = torch.Tensor([[1] * char_len] * char_len)

            Graph = torch.LongTensor(G[i])
            for j in range(4):
                G_bitcher[i, j, :char_lens[i], :word_lens[i]] = Graph.eq(j + 1)
        if self.use_gpu:
            char_bitcher = char_bitcher.cuda()
            word_bitcher = word_bitcher.cuda()
            if self.char_emb_file == 'bert' or self.use_bert == True:
                bert_bitcher = bert_bitcher.cuda()
                bert_mask = bert_mask.cuda()
            label_bitcher = label_bitcher.cuda()
            mask_bitcher = mask_bitcher.cuda()
            G_bitcher = G_bitcher.cuda()
            self_mask_bitcher = self_mask_bitcher.cuda()
        return char_bitcher, word_bitcher, bert_bitcher, bert_mask, label_bitcher, mask_bitcher, G_bitcher, self_mask_bitcher

    def lookup_char_id(self, char):
        if char not in self.char2id:
            if char not in self.char_dict:
                if char.lower() in self.char_dict:
                    char_emb = self.char_dict[char.lower()]
                elif char.upper() in self.char_dict:
                    char_emb = self.char_dict[char.upper()]
                else:
                    print(char + ' is oov')
                    scale = np.sqrt(3.0 / self.d_char)
                    char_emb = np.random.uniform(-scale, scale, [self.d_char]).tolist()
                    self.char_dict[char] = char_emb
            else:
                char_emb = self.char_dict[char]
            self.pretrain_char_emb.append(char_emb)
            char_size = len(self.char2id)
            self.char2id[char] = char_size
            self.id2char[char_size] = char
        return self.char2id[char]

    def lookup_word_id(self, word):
        if word not in self.word2id:
            if word not in self.word_dict:
                if word.lower() in self.word_dict:
                    word_emb = self.word_dict[word.lower()]
                elif word.upper() in self.word_dict:
                    word_emb = self.word_dict[word.upper()]
                else:
                    print(word + ' is oov')
                    scale = np.sqrt(3.0 / self.d_word)
                    word_emb = np.random.uniform(-scale, scale, [self.d_word]).tolist()
            else:
                word_emb = self.word_dict[word]
            self.pretrain_word_emb.append(word_emb)
            word_size = len(self.word2id)
            self.word2id[word] = word_size
            self.id2word[word_size] = word
        return self.word2id[word]

    def clear_cache(self):
        self.char_dict = None
        self.word_dict = None
        self.tire = None
        self.bert_tokenizer = None

    def ctgtonum(self, category):
        return self.ctg_dic.index(category)

    def decodeCtg(self, ctg):
        return self.ctg_dic[ctg]

    def write_file(self, filepath, data):
        with open(filepath, 'wb') as fp:
            pickle.dump(data, fp)

    def read_file(self, filepath):
        with open(filepath, "rb") as fp:
            return pickle.load(fp)

    def get_candidate_word(self, data):
        result = []
        for i in range(len(data)):
            result.extend(self.trie.enumerateMatch(data[i:], ''))
        return result

    def process_file_to_id(self, filepath):
        if os.path.exists(filepath) == False:
            return None, None, None, None, None, None, None
        if self.bert_tokenizer == None:
            self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', do_lower_case=True)

        if self.char_dict == None:
            self.get_emb_dic_by_gaz()

        if self.trie == None:
            self.get_trie()
            print('Successful get Trie and LexiconÔºÅ')

        data, label = self.load_file(filepath)
        chars = []
        bert_ids = []
        words = []
        G = []
        char_lens = []
        word_lens = []
        print('processing file: ', filepath)
        for x in range(len(data)):
            if len(data[x]) > self.maxSentenceLength:
                data[x] = data[x][0:self.maxSentenceLength]
            char_size = len(data[x])
            total_words = self.get_candidate_word(data[x])  # candidate_word
            word_size = len(total_words)

            char_lens.append(char_size)
            word_lens.append(word_size)
            if self.char_emb_file != 'bert':
                char_id = [self.lookup_char_id(data[x][i]) for i in range(char_size)]
                chars.append(char_id)
            else:
                chars.append([])
            word_id = [self.lookup_word_id(total_words[i]) for i in range(word_size)]
            words.append(word_id)

            bert_id = self.bert_tokenizer.convert_tokens_to_ids(['[CLS]'] + data[x] + ['[SEP]'])
            bert_ids.append(bert_id)

            # Build Graph(char_size,word_size)
            temp_G = np.zeros((char_size, word_size), dtype=int).tolist()
            for y in range(char_size):
                temp_word = self.trie.enumerateMatch(data[x][y:], '')
                for word in temp_word:
                    word_length = len(word)
                    word_index = total_words.index(word)
                    if word_length == 1:  # S entity--mark 4
                        temp_G[y][word_index] = 4
                    else:
                        temp_G[y][word_index] = 1  # B entity--mark 1
                        temp_G[y + word_length - 1][word_index] = 3  # E entity--mark 3
                        for temp_index in range(1, word_length - 1):
                            temp_G[y + temp_index][word_index] = 2  # M entity--mark 2
            G.append(temp_G)

        return chars, words, bert_ids, label, G, char_lens, word_lens

    def get_input_data(self):
        file = self.input_data_path + self.source_type + '/'
        if os.path.exists(file) == False:
            os.makedirs(file)
        file = file + 'processed_file_char' + str(self.d_char) + '_word' + str(self.d_word) + '.pkl'
        if os.path.exists(file) == False:  # Êñá‰ª∂‰∏çÂ≠òÂú®

            chars_train, words_train, bert_train, y_train, G_train, char_lens_train, word_lens_train = self.process_file_to_id(
                self.resource_path + self.source_type + '/train.bmes')
            chars_dev, words_dev, bert_dev, y_dev, G_dev, char_lens_dev, word_lens_dev = self.process_file_to_id(
                self.resource_path + self.source_type + '/dev.bmes')
            chars_test, words_test, bert_test, y_test, G_test, char_lens_test, word_lens_test = self.process_file_to_id(
                self.resource_path + self.source_type + '/test.bmes')

            data = [chars_train, words_train, bert_train, y_train, G_train, char_lens_train, word_lens_train, chars_dev,
                    words_dev, bert_dev, y_dev, G_dev, char_lens_dev, word_lens_dev, chars_test, words_test, bert_test,
                    y_test, G_test, char_lens_test, word_lens_test, self.char2id, self.id2char, self.pretrain_char_emb,
                    self.word2id, self.id2word, self.pretrain_word_emb]
            self.write_file(file, data)

            return data
        else:
            print("loading processed data...")
            data = self.read_file(file)
            return data

    def predict_check(self, pred_variable, gold_variable, mask_variable):
        """
            input:
                pred_variable (batch_size, sent_len): pred tag result, in numpy format
                gold_variable (batch_size, sent_len): gold result variable
                mask_variable (batch_size, sent_len): mask variable
        """

        pred = pred_variable.cpu().data.numpy()
        gold = gold_variable.cpu().data.numpy()
        mask = mask_variable.cpu().data.numpy()
        overlaped = (pred == gold)
        right_token = np.sum(overlaped * mask)
        total_token = mask.sum()

        return right_token, total_token

    def get_score(self, predict_list, gold_list, lens):
        '''
            Precission=right_num/gold_num
            Recall=right_num/predict_num
            F_1=2*P*R/(P+R)
        '''

        def reverse_style(input_string):
            target_position = input_string.index('[')
            input_len = len(input_string)
            output_string = input_string[target_position:input_len] + input_string[0:target_position]
            return output_string

        def get_ner_BMES(label_list):
            # list_len = len(word_list)
            # assert(list_len == len(label_list)), "word list size unmatch with label list"
            list_len = len(label_list)
            begin_label = 'B-'
            end_label = 'E-'
            single_label = 'S-'
            whole_tag = ''
            index_tag = ''
            tag_list = []
            stand_matrix = []
            for i in range(0, list_len):
                # wordlabel = word_list[i]
                current_label = label_list[i].upper() if label_list[i] else []
                if begin_label in current_label:
                    if index_tag != '':
                        tag_list.append(whole_tag + ',' + str(i - 1))
                    whole_tag = current_label.replace(begin_label, "", 1) + '[' + str(i)
                    index_tag = current_label.replace(begin_label, "", 1)

                elif single_label in current_label:
                    if index_tag != '':
                        tag_list.append(whole_tag + ',' + str(i - 1))
                    whole_tag = current_label.replace(single_label, "", 1) + '[' + str(i)
                    tag_list.append(whole_tag)
                    whole_tag = ""
                    index_tag = ""
                elif end_label in current_label:
                    if index_tag != '':
                        tag_list.append(whole_tag + ',' + str(i))
                    whole_tag = ''
                    index_tag = ''
                else:
                    continue
            if (whole_tag != '') & (index_tag != ''):
                tag_list.append(whole_tag)
            tag_list_len = len(tag_list)

            for i in range(0, tag_list_len):
                if len(tag_list[i]) > 0:
                    tag_list[i] = tag_list[i] + ']'
                    insert_list = reverse_style(tag_list[i])
                    stand_matrix.append(insert_list)
            # print stand_matrix
            return stand_matrix

        def get_ner_BIO(label_list):
            # list_len = len(word_list)
            # assert(list_len == len(label_list)), "word list size unmatch with label list"
            list_len = len(label_list)
            begin_label = 'B-'
            inside_label = 'I-'
            whole_tag = ''
            index_tag = ''
            tag_list = []
            stand_matrix = []
            for i in range(0, list_len):
                # wordlabel = word_list[i]
                current_label = label_list[i].upper()
                if begin_label in current_label:
                    if index_tag == '':
                        whole_tag = current_label.replace(begin_label, "", 1) + '[' + str(i)
                        index_tag = current_label.replace(begin_label, "", 1)
                    else:
                        tag_list.append(whole_tag + ',' + str(i - 1))
                        whole_tag = current_label.replace(begin_label, "", 1) + '[' + str(i)
                        index_tag = current_label.replace(begin_label, "", 1)

                elif inside_label in current_label:
                    if current_label.replace(inside_label, "", 1) == index_tag:
                        whole_tag = whole_tag
                    else:
                        if (whole_tag != '') & (index_tag != ''):
                            tag_list.append(whole_tag + ',' + str(i - 1))
                        whole_tag = ''
                        index_tag = ''
                else:
                    if (whole_tag != '') & (index_tag != ''):
                        tag_list.append(whole_tag + ',' + str(i - 1))
                    whole_tag = ''
                    index_tag = ''

            if (whole_tag != '') & (index_tag != ''):
                tag_list.append(whole_tag)
            tag_list_len = len(tag_list)

            for i in range(0, tag_list_len):
                if len(tag_list[i]) > 0:
                    tag_list[i] = tag_list[i] + ']'
                    insert_list = reverse_style(tag_list[i])
                    stand_matrix.append(insert_list)
            return stand_matrix

        def get_ner_fmeasure(golden_lists, predict_lists, label_type="BMES"):
            sent_num = len(golden_lists)
            golden_full = []
            predict_full = []
            right_full = []
            right_tag = 0
            all_tag = 0
            for idx in range(0, sent_num):
                # word_list = sentence_lists[idx]
                golden_list = golden_lists[idx]
                predict_list = predict_lists[idx]
                for idy in range(len(golden_list)):
                    if golden_list[idy] == predict_list[idy]:
                        right_tag += 1
                all_tag += len(golden_list)
                if label_type == "BMES":
                    gold_matrix = get_ner_BMES(golden_list)
                    pred_matrix = get_ner_BMES(predict_list)
                else:
                    gold_matrix = get_ner_BIO(golden_list)
                    pred_matrix = get_ner_BIO(predict_list)
                # print "gold", gold_matrix
                # print "pred", pred_matrix
                right_ner = list(set(gold_matrix).intersection(set(pred_matrix)))
                golden_full += gold_matrix
                predict_full += pred_matrix
                right_full += right_ner
            right_num = len(right_full)
            golden_num = len(golden_full)
            predict_num = len(predict_full)
            if predict_num == 0:
                precision = -1
            else:
                precision = (right_num + 0.0) / predict_num
            if golden_num == 0:
                recall = -1
            else:
                recall = (right_num + 0.0) / golden_num
            if (precision == -1) or (recall == -1) or (precision + recall) <= 0.:
                f_measure = -1
            else:
                f_measure = 2 * precision * recall / (precision + recall)
            accuracy = (right_tag + 0.0) / all_tag
            return golden_num, predict_num, right_num, accuracy, precision, recall, f_measure

        golden_lists = []
        predict_lists = []
        for id_sent in range(len(lens)):
            golden_lists.append([self.decodeCtg(gold_list[id_sent][j]) for j in range(lens[id_sent])])
            predict_lists.append([self.decodeCtg(predict_list[id_sent][j]) for j in range(lens[id_sent])])

        golden_num, predict_num, right_num, acc, p, r, f = get_ner_fmeasure(golden_lists, predict_lists, self.tagScheme)

        return golden_num, predict_num, right_num, acc, p, r, f

    def draw_pic(self, P, R, F):
        pic_path = './result/' + self.source_type + '/'
        if os.path.exists(pic_path) == False:
            os.makedirs(pic_path)
        x = [i for i in range(len(P))]
        fig = pl.figure(figsize=(6, 6))
        pl.plot(x, P, ms=10, label='p')
        pl.plot(x, R, ms=10, label='r')
        pl.plot(x, F, ms=10, label='f')
        pl.legend(loc='upper left')
        fig.savefig(pic_path + self.source_type + '.jpg', dpi=600, bbox_inches='tight')
        # pl.show()

    def get_metrics(self, predict, target, lens):
        metrics_path = './result/' + self.source_type + '/'
        if os.path.exists(metrics_path) == False:
            os.makedirs(metrics_path)
        predict_list = []
        target_list = []
        for i in range(len(lens)):
            for j in range(lens[i]):
                predict_list.append(self.ctg_dic[predict[i][j]])
                target_list.append(self.ctg_dic[target[i][j]])
        if len(predict_list) != len(target_list):
            print('predict list and gold list have different lengthsÔºÅ')
        else:
            report = metrics.classification_report(predict_list, target_list, zero_division=0)
            with open(metrics_path + self.source_type + '_metrics.txt', 'w', encoding='utf-8') as fp:
                fp.write(report)
