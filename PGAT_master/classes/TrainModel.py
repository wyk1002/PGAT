import numpy as np
import torch
import torch.nn as nn
from classes.crf import CRF
from transformers import BertModel
from .Layers import PGAT_layer, LayerNorm, CNN_Model, Transformer_Model


class PGAT_Model(nn.Module):
    def __init__(self, d_char, d_word, hidden_dim, dropout, att_dropout, trust_rank, layer_num, category_num,
                 seq_encoding_model, pgat_head, seq_head, att_head, context_feature_model, seq_feature_model, use_gpu,
                 use_bert, is_char_use_bert, pretrain_char, pretrain_word):
        super(PGAT_Model, self).__init__()
        self.layer_num = layer_num
        self.use_bert = use_bert
        self.is_char_use_bert = is_char_use_bert
        self.seq_encoding_model = seq_encoding_model

        if is_char_use_bert == False:
            pretrain_char[0] = np.zeros([d_char]).tolist()
            self.char_embedding = nn.Embedding(len(pretrain_char), d_char)
            self.char_embedding.weight.data.copy_(torch.from_numpy(np.array(pretrain_char)))
            # for p in self.char_embedding.parameters():
            #     p.requires_grad = False
            if use_gpu:
                self.char_embedding = self.char_embedding.cuda()

        pretrain_word[0] = np.zeros([d_word]).tolist()
        self.word_embedding = nn.Embedding(len(pretrain_word), d_word)
        self.word_embedding.weight.data.copy_(torch.from_numpy(np.array(pretrain_word)))
        # for p in self.word_embedding.parameters():
        #     p.requires_grad = False

        self.drop = nn.Dropout(p=dropout)

        self.pgat_layer = PGAT_layer(d_char, d_word, dropout=att_dropout, head=pgat_head)
        input_dim = d_char + 4 * d_word

        if use_bert:
            self.bert_encoder = BertModel.from_pretrained('bert-base-chinese')
            input_dim += 768
            for p in self.bert_encoder.parameters():
                p.requires_grad = False
            if use_gpu:
                self.bert_encoder = self.bert_encoder.cuda()

        if seq_encoding_model == 'lstm':
            self.encoding_layer = nn.LSTM(input_dim, hidden_dim, num_layers=layer_num, batch_first=True,
                                          bidirectional=True,)
            self.relu = nn.ReLU()
            self.dropout_layer = nn.Dropout(0.5)
            hidden_dim = 2 * hidden_dim
        elif seq_encoding_model == 'cnn':
            self.encoding_layer = CNN_Model(input_dim,hidden_dim,layer_num,dropout,use_gpu)
        elif seq_encoding_model == 'transformer':
            self.encoding_layer = Transformer_Model(input_dim,hidden_dim, att_head,att_dropout,layer_num)
        elif seq_encoding_model == None:
            hidden_dim = input_dim
        else:
            print('seq_encoding_model is wrong!')
            exit(0)

        self.layernormal = LayerNorm(hidden_dim)
        self.W_reshape = nn.Linear(hidden_dim, category_num + 2)

        self.crf = CRF(category_num, use_gpu)

        if use_gpu:
            self.word_embedding = self.word_embedding.cuda()
            self.pgat_layer = self.pgat_layer.cuda()
            self.encoding_layer = self.encoding_layer.cuda()
            self.layernormal = self.layernormal.cuda()
            self.W_reshape = self.W_reshape.cuda()
            self.crf = self.crf.cuda()

    def get_hidden_state(self, chars, words, bert, bert_mask, G, lens_mask,self_mask_matrix):
        '''

                :param chars: (b,char_len)
                :param words: (b,word_len)
                :param bert: (b,char_len)
                :param bert_mask (b,char_len+2)
                :param G:(b,char_len,word_len)
                :param lens_mask: (b,char_len)
                :param self_mask_matrix: (b,char_len,char_len)
                :return (b,char_len,tag_num+2)
        '''
        if self.use_bert or self.is_char_use_bert:
            seg_id = torch.zeros(bert_mask.size()).long().cuda()
            outputs = self.bert_encoder(bert, bert_mask, seg_id)
            bert_emb = outputs[0][:, 1:-1, :]

        if self.is_char_use_bert:
            char_emb = bert_emb
        else:
            char_emb = self.char_embedding(chars)
        word_emb = self.word_embedding(words)

        if self.seq_encoding_model != 'transformer':
            char_emb = self.drop(char_emb)
            word_emb = self.drop(word_emb)

        x = self.pgat_layer(char_emb, word_emb, G)  # (b,char_len,d_c+4*d_w)

        if self.use_bert:
            x = torch.cat([x, bert_emb], dim=-1)  # (b,char_len,d_c+4*d_w+768)

        if self.seq_encoding_model == 'transformer':
            x=self.encoding_layer(x,self_mask_matrix)
        if self.seq_encoding_model == 'lstm':
            x, hidden = self.encoding_layer(x, None)
            # x = self.relu(x)
            # x = self.layernormal(x)
            x = self.dropout_layer(x)
        if self.seq_encoding_model == 'cnn':
            x=self.encoding_layer(x)

        x = self.W_reshape(x)  # (b,char_len,tag_num+2)
        return x

    def call(self, chars, words, bert, bert_mask, label, self_mask_matrix, lens_mask, G):
        '''

        :param chars: (b,char_len)
        :param words: (b,word_len)
        :param bert: (b,char_len)
        :param bert_mask: (b,char_len+2)
        :param label: (b,char_len)
        :param self_mask_matrix: (b,char_len,char_len)
        :param lens_mask: (b,char_len)
        :param G:(b,char_len,word_len)
        :return: loss, predict
        '''

        x = self.get_hidden_state(chars, words, bert, bert_mask, G, lens_mask,self_mask_matrix)
        loss = self.crf.neg_log_likelihood_loss(x, lens_mask.bool(), label)
        scores, predict = self.crf._viterbi_decode(x, lens_mask.bool())
        return loss, predict

    def forward(self, chars, words, bert, bert_mask, self_mask_matrix, lens_mask, G):
        '''

        :param chars: (b,char_len)
        :param words: (b,word_len)
        :param bert: (b,char_len)
        :param bert_mask: (b,char_len+2)
        :param label: (b,char_len)
        :param self_mask_matrix: (b,char_len,char_len)
        :param lens_mask: (b,char_len)
        :param G:(b,char_len,word_len)
        :return: loss, predict
        '''

        x = self.get_hidden_state(chars, words, bert, bert_mask, G, lens_mask,self_mask_matrix)
        # loss = self.crf.neg_log_likelihood_loss(x, lens_mask.bool(), label)
        scores, predict = self.crf._viterbi_decode(x, lens_mask.bool())
        return predict
